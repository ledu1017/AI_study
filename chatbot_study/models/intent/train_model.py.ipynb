{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e42200af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0800511",
   "metadata": {},
   "source": [
    "pandas: pd로 별칭 지정한 패키지로, 데이터 조작과 분석에 유용한 기능을 제공하는 라이브러리입니다. 주로 표 형태의 데이터를 다룰 때 사용되며, 데이터프레임(DataFrame)이라는 자료구조를 제공합니다.\n",
    "\n",
    "tensorflow: tf로 별칭 지정한 딥러닝 프레임워크로, 기계 학습과 딥러닝 모델을 구축하고 학습시키는 데 사용됩니다. TensorFlow는 텐서(Tensor)라는 다차원 배열을 다루는 라이브러리로, 그래프 기반의 연산을 수행합니다.\n",
    "\n",
    "tensorflow.keras: TensorFlow에서 제공하는 고수준 딥러닝 API로, Keras API를 TensorFlow에 통합한 것입니다. 딥러닝 모델을 쉽게 구축하고 학습시킬 수 있는 편리한 인터페이스를 제공합니다.\n",
    "\n",
    "preprocessing: tensorflow.keras.preprocessing 모듈은 데이터 전처리에 사용됩니다. 주로 텍스트, 이미지, 시계열 데이터 등을 신경망 모델에 입력하기 전에 사전 처리하는 데 사용됩니다. 예를 들어, 텍스트 데이터의 토큰화, 정수 인코딩, 패딩 등의 작업을 처리할 수 있습니다.\n",
    "\n",
    "tensorflow.keras.models: TensorFlow에서 제공하는 모델 구성을 위한 모듈입니다. 주요 모델 구성 요소인 층(layer)들을 조합하여 모델을 생성하고 관리하는 기능을 제공합니다.\n",
    "\n",
    "tensorflow.keras.layers: TensorFlow에서 제공하는 다양한 종류의 층(layer)들을 포함하는 모듈입니다. 예를 들어, Dense 층은 완전 연결 신경망(fully connected neural network)을 구성하는 데 사용되며, Conv1D 층은 1차원 합성곱 신경망(convolutional neural network)을 구성하는 데 사용됩니다. Embedding 층은 텍스트나 범주형 데이터를 처리하는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad124857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 읽어오기\n",
    "train_file = \"total_train_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f774e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(train_file, delimiter = ',')\n",
    "queries = data['query'].tolist()\n",
    "intents = data['intent'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "674b85ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys    # 다른 폴더에 있는것들 가져오기 위함\n",
    "\n",
    "module_paths = [\n",
    "    'C:\\\\Users\\\\ledu2\\\\NLP\\\\chatbot_study\\\\utils',\n",
    "    'C:\\\\Users\\\\ledu2\\\\NLP\\\\chatbot_study\\\\config'\n",
    "]\n",
    "\n",
    "for path in module_paths:\n",
    "    abs_path = os.path.abspath(path)\n",
    "    if abs_path not in sys.path:\n",
    "        sys.path.append(abs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2219076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Preprocess import Preprocess\n",
    "p = Preprocess(word2index_dic = '../../train_tools/dict/chatbot_dict.bin', \n",
    "              userdic = '../../utils/user_dic.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cef19b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 시퀀스 생성\n",
    "sequences = []\n",
    "for sentence in queries:\n",
    "    pos = p.pos(sentence)\n",
    "    keywords = p.get_keywords(pos, without_tag = True)\n",
    "    seq = p.get_wordidx_sequence(keywords)\n",
    "    sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5cb19d78",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MAX_SEQ_LEN' from 'GlobalParams' (C:\\Users\\ledu2\\NLP\\chatbot_study\\config\\GlobalParams.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 단어 인덱스 시퀀스 벡터 생성\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# 단어 시퀀스 백터 크기\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mGlobalParams\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MAX_SEQ_LEN\n\u001b[0;32m      4\u001b[0m padded_seqs \u001b[38;5;241m=\u001b[39m preprocessing\u001b[38;5;241m.\u001b[39msequence\u001b[38;5;241m.\u001b[39mpad_sequences(sequences, maxlen \u001b[38;5;241m=\u001b[39m MAX_SEQ_LEN,\n\u001b[0;32m      5\u001b[0m                                                   padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'MAX_SEQ_LEN' from 'GlobalParams' (C:\\Users\\ledu2\\NLP\\chatbot_study\\config\\GlobalParams.py)"
     ]
    }
   ],
   "source": [
    "# 단어 인덱스 시퀀스 벡터 생성\n",
    "# 단어 시퀀스 백터 크기\n",
    "from GlobalParams import MAX_SEQ_LEN\n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen = MAX_SEQ_LEN,\n",
    "                                                  padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9825097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용, 검증용, 테스트용 데이터셋 생성\n",
    "# 학습셋:검증셋:테스트셋 = 7:2:1\n",
    "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, intents))\n",
    "ds = ds.shuffle(len(queries))\n",
    "\n",
    "tran_size = int(len(padded_seqs) * 0.7)\n",
    "val_size = int(len(padded_seqs) * 0.2)\n",
    "test_size = int(len(padded_seqs) * 0.1)\n",
    "\n",
    "train_ds = ds.take(train_size).batch(20)\n",
    "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
    "test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ffb21d",
   "metadata": {},
   "source": [
    "\n",
    "tf.data.Dataset.from_tensor_slices((padded_seqs, intents)):\n",
    "\n",
    "padded_seqs: 패딩된 시퀀스 데이터를 담고 있는 텐서입니다. 시퀀스 데이터는 문장이나 문서를 숫자로 변환한 것을 말합니다.\n",
    "intents: 각 시퀀스 데이터에 대응하는 의도(라벨) 정보를 담고 있는 텐서입니다.\n",
    "from_tensor_slices() 메서드를 사용하여 텐서를 슬라이스하고 데이터셋을 생성합니다. 이렇게 하면 시퀀스 데이터와 의도 정보가 쌍으로 매칭된 데이터셋이 생성됩니다.\n",
    "ds.shuffle(len(queries)):\n",
    "\n",
    "ds: 생성한 데이터셋 객체입니다.\n",
    "shuffle(len(queries)) 메서드를 사용하여 데이터셋을 섞습니다. len(queries)는 데이터셋의 총 개수를 의미합니다. 데이터셋을 섞는 것은 모델이 데이터의 순서를 기억하지 못하도록 하고, 더 좋은 학습 결과를 얻기 위해 데이터를 무작위로 배치하는데 도움을 줍니다.\n",
    "이 코드들을 실행하면 입력 데이터를 TensorFlow의 데이터셋으로 구성하고, 데이터셋을 섞어서 모델에 사용할 준비를 완료합니다. 이후에는 데이터셋을 배치하거나 반복하여 모델에 입력으로 제공할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877cab5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
