{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e42200af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce65f17",
   "metadata": {},
   "source": [
    "pandas: pd로 별칭 지정한 패키지로, 데이터 조작과 분석에 유용한 기능을 제공하는 라이브러리입니다. 주로 표 형태의 데이터를 다룰 때 사용되며, 데이터프레임(DataFrame)이라는 자료구조를 제공합니다.\n",
    "\n",
    "tensorflow: tf로 별칭 지정한 딥러닝 프레임워크로, 기계 학습과 딥러닝 모델을 구축하고 학습시키는 데 사용됩니다. TensorFlow는 텐서(Tensor)라는 다차원 배열을 다루는 라이브러리로, 그래프 기반의 연산을 수행합니다.\n",
    "\n",
    "tensorflow.keras: TensorFlow에서 제공하는 고수준 딥러닝 API로, Keras API를 TensorFlow에 통합한 것입니다. 딥러닝 모델을 쉽게 구축하고 학습시킬 수 있는 편리한 인터페이스를 제공합니다.\n",
    "\n",
    "preprocessing: tensorflow.keras.preprocessing 모듈은 데이터 전처리에 사용됩니다. 주로 텍스트, 이미지, 시계열 데이터 등을 신경망 모델에 입력하기 전에 사전 처리하는 데 사용됩니다. 예를 들어, 텍스트 데이터의 토큰화, 정수 인코딩, 패딩 등의 작업을 처리할 수 있습니다.\n",
    "\n",
    "tensorflow.keras.models: TensorFlow에서 제공하는 모델 구성을 위한 모듈입니다. 주요 모델 구성 요소인 층(layer)들을 조합하여 모델을 생성하고 관리하는 기능을 제공합니다.\n",
    "\n",
    "tensorflow.keras.layers: TensorFlow에서 제공하는 다양한 종류의 층(layer)들을 포함하는 모듈입니다. 예를 들어, Dense 층은 완전 연결 신경망(fully connected neural network)을 구성하는 데 사용되며, Conv1D 층은 1차원 합성곱 신경망(convolutional neural network)을 구성하는 데 사용됩니다. Embedding 층은 텍스트나 범주형 데이터를 처리하는 데 사용됩니다.\n",
    "\n",
    "Input 레이어는 신경망 모델의 입력 데이터를 정의하는 역할을 합니다. 신경망은 데이터를 입력받아 여러 계층을 거쳐 출력을 생성하는 구조로 이루어져 있습니다. Input 레이어는 이러한 입력 데이터를 정의하고, 모델의 첫 번째 레이어로 사용됩니다.\n",
    "여기서 shape은 입력 데이터의 형태를 정의하는 매개변수입니다. 입력 데이터의 모양(shape)은 일반적으로 (batch_size, input_shape)의 형태를 갖습니다. input_shape은 데이터의 차원(dimension)을 나타내는 튜플이며, 예를 들어 이미지의 경우 (height, width, channels)와 같은 형태가 될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad124857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 읽어오기\n",
    "train_file = \"total_train_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f774e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(train_file, delimiter = ',')\n",
    "queries = data['query'].tolist()\n",
    "intents = data['intent'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a31131c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys    # 다른 폴더에 있는것들 가져오기 위함\n",
    "\n",
    "module_paths = [\n",
    "    'C:\\\\Users\\\\ledu2\\\\NLP\\\\chatbot_study\\\\utils',\n",
    "    'C:\\\\Users\\\\ledu2\\\\NLP\\\\chatbot_study\\\\config'\n",
    "]\n",
    "\n",
    "for path in module_paths:\n",
    "    abs_path = os.path.abspath(path)\n",
    "    if abs_path not in sys.path:\n",
    "        sys.path.append(abs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2219076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Preprocess import Preprocess\n",
    "p = Preprocess(word2index_dic = '../../train_tools/dict/chatbot_dict.bin', \n",
    "              userdic = '../../utils/user_dic.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cef19b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 시퀀스 생성\n",
    "sequences = []\n",
    "for sentence in queries:\n",
    "    pos = p.pos(sentence)\n",
    "    keywords = p.get_keywords(pos, without_tag = True)\n",
    "    seq = p.get_wordidx_sequence(keywords)\n",
    "    sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cb19d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 인덱스 시퀀스 벡터 생성\n",
    "# 단어 시퀀스 백터 크기\n",
    "from GlobalParams import MAX_SEQ_LEN\n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen = MAX_SEQ_LEN,\n",
    "                                                  padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cfe5f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용, 검증용, 테스트용 데이터셋 생성\n",
    "# 학습셋:검증셋:테스트셋 = 7:2:1\n",
    "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, intents))\n",
    "ds = ds.shuffle(len(queries))\n",
    "\n",
    "train_size = int(len(padded_seqs) * 0.7)\n",
    "val_size = int(len(padded_seqs) * 0.2)\n",
    "test_size = int(len(padded_seqs) * 0.1)\n",
    "\n",
    "train_ds = ds.take(train_size).batch(20)\n",
    "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
    "test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1614c081",
   "metadata": {},
   "source": [
    "\n",
    "tf.data.Dataset.from_tensor_slices((padded_seqs, intents)):\n",
    "\n",
    "padded_seqs: 패딩된 시퀀스 데이터를 담고 있는 텐서입니다. 시퀀스 데이터는 문장이나 문서를 숫자로 변환한 것을 말합니다.\n",
    "intents: 각 시퀀스 데이터에 대응하는 의도(라벨) 정보를 담고 있는 텐서입니다.\n",
    "from_tensor_slices() 메서드를 사용하여 텐서를 슬라이스하고 데이터셋을 생성합니다. 이렇게 하면 시퀀스 데이터와 의도 정보가 쌍으로 매칭된 데이터셋이 생성됩니다.\n",
    "ds.shuffle(len(queries)):\n",
    "\n",
    "ds: 생성한 데이터셋 객체입니다.\n",
    "shuffle(len(queries)) 메서드를 사용하여 데이터셋을 섞습니다. len(queries)는 데이터셋의 총 개수를 의미합니다. 데이터셋을 섞는 것은 모델이 데이터의 순서를 기억하지 못하도록 하고, 더 좋은 학습 결과를 얻기 위해 데이터를 무작위로 배치하는데 도움을 줍니다.\n",
    "이 코드들을 실행하면 입력 데이터를 TensorFlow의 데이터셋으로 구성하고, 데이터셋을 섞어서 모델에 사용할 준비를 완료합니다. 이후에는 데이터셋을 배치하거나 반복하여 모델에 입력으로 제공할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3d3ca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정 \n",
    "dropout_prob = 0.5\n",
    "EMB_SIZE = 128\n",
    "EPOCH = 5\n",
    "VOCAB_SIZE = len(p.word_index)+1    # 전체 단어 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1dec2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 모델 정의\n",
    "input_layer = Input(shape=(MAX_SEQ_LEN,))\n",
    "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_SEQ_LEN)(input_layer)\n",
    "dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6826de5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = Conv1D(\n",
    "        filters = 128,\n",
    "        kernel_size = 3,\n",
    "        padding = 'valid',    # valid 패딩을 하면 결과는 항상 입력보다 작아짐\n",
    "        activation = tf.nn.relu)(dropout_emb)\n",
    "pool1 = GlobalMaxPool1D()(conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a06c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2 = Conv1D(\n",
    "        filters = 128,\n",
    "        kernel_size = 4,\n",
    "        padding = 'valid',\n",
    "        activation = tf.nn.relu)(dropout_emb)\n",
    "pool2 = GlobalMaxPool1D()(conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c02d4327",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv3 = Conv1D(\n",
    "        filters = 128,\n",
    "        kernel_size = 5,\n",
    "        padding = 'valid',\n",
    "        activation = tf.nn.relu)(dropout_emb)\n",
    "pool3 = GlobalMaxPool1D()(conv3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5f43956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3, 4, 5-gram 이후 합치기\n",
    "concat = concatenate([pool1, pool2, pool3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5868067",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = Dense(128, activation = tf.nn.relu)(concat)\n",
    "dropout_hidden = Dropout(rate=dropout_prob)(hidden)\n",
    "logits = Dense(5, name='logits')(dropout_hidden)\n",
    "predictions = Dense(5, activation = tf.nn.softmax)(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37f257e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "#이진 분류의 문제에서는 주로 binary_crossentropy를 사용하고,\n",
    "#다중 분류에서는 categorical_crossentropy, \n",
    "#수치형 변수를 예측할 때는 mean_squared_error를 주로 사용\n",
    "model = Model(inputs = input_layer, outputs = predictions)\n",
    "model.compile(optimizer = 'adam',\n",
    "             loss = 'sparse_categorical_crossentropy',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6107777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3698/3698 [==============================] - 173s 47ms/step - loss: 0.0482 - accuracy: 0.9854 - val_loss: 0.0157 - val_accuracy: 0.9934\n",
      "Epoch 2/5\n",
      "3698/3698 [==============================] - 189s 51ms/step - loss: 0.0156 - accuracy: 0.9946 - val_loss: 0.0089 - val_accuracy: 0.9957\n",
      "Epoch 3/5\n",
      "3698/3698 [==============================] - 195s 53ms/step - loss: 0.0120 - accuracy: 0.9952 - val_loss: 0.0076 - val_accuracy: 0.9969\n",
      "Epoch 4/5\n",
      "3698/3698 [==============================] - 201s 54ms/step - loss: 0.0096 - accuracy: 0.9961 - val_loss: 0.0071 - val_accuracy: 0.9967\n",
      "Epoch 5/5\n",
      "3698/3698 [==============================] - 200s 54ms/step - loss: 0.0073 - accuracy: 0.9968 - val_loss: 0.0052 - val_accuracy: 0.9975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x272143ffbb0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습\n",
    "# verbose 0 은 출력하지 않고, 1은 자세히, 2는 함축적인 정보\n",
    "\n",
    "model.fit(train_ds, validation_data = val_ds, epochs = EPOCH, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9242bd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "529/529 [==============================] - 1s 2ms/step - loss: 0.0065 - accuracy: 0.9975\n",
      "Accuracy: 99.753904\n",
      "loss: 0.006529\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가(테스트 데이터셋 이용)\n",
    "loss, accuracy = model.evaluate(test_ds, verbose = 1)\n",
    "print('Accuracy: %f' % (accuracy  * 100))\n",
    "print('loss: %f' % (loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79f47863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "model.save('intent_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4cc554",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
